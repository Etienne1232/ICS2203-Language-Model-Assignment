{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICS2203 â€“ Statistical Natural Language Processing\n",
    "## Building a Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting data from the corpus."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading raw data from the Download files provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_directory = './Data/Texts/'\n",
    "all_data = []\n",
    "\n",
    "#appends all files to all_data\n",
    "for x in os.listdir(file_directory):\n",
    "    for y in os.listdir(file_directory + x):\n",
    "        all_data.append(ET.parse(file_directory+x+'/'+y))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating through every xml file in the dataset and extracts all text from the files. <br>\n",
    "The data is saved in a 2D list. Every sub-list in the main list will contain 1 sentence. Every element in the sublist will contain 1 word or punctuation mark. <br>\n",
    "Some words ended with a space in the corpus, so I made sure to negate that space so that, for example: 'hello' and 'hello ' count as the same word. <br>\n",
    "I added start- and end-of-sentence tags to ease computation later on. <br> The dataset was then shuffled and a random 15% of the dataset was taken to be the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(data):\n",
    "    dataset = []\n",
    "    for file in data:\n",
    "        for tree in file.getroot():\n",
    "            for sentence in tree.iter('s'):\n",
    "                temp_sentence = ['<s>']\n",
    "                for word in sentence:\n",
    "                        if word.text is not None:\n",
    "                            if word.text[-1] == ' ':\n",
    "                                temp_sentence.append(word.text[:-1])\n",
    "                            else:\n",
    "                                temp_sentence.append(word.text)\n",
    "                temp_sentence.append(\"</s>\")\n",
    "                dataset.append(temp_sentence)\n",
    "\n",
    "    random.shuffle(dataset)\n",
    "    slicing_point = int(len(dataset) * 0.85)\n",
    "    return dataset[:slicing_point], dataset[slicing_point:] "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating N-Grams"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function takes a dataset, the n (amount of elements in the n-gram), and if, provided, it takes a dictionary with already defined words and frequencies. <br>\n",
    "This method iterates through every sentence in the training dataset and generates n-grams for a provided n. <br>\n",
    "These n-grams are then tallied into a dictionary, where every n-gram is saved as a key, and the amount of times it appears is its value. <br>\n",
    "This dictionary is then returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_n_gram(data, n, frequencies = {}):\n",
    "    for sentence in data:\n",
    "        #len(sentence)-n+1 was taken as for each n-gram, there are that many windows of words(grams)\n",
    "        temp_ngrams = [tuple(sentence[x:x+n]) for x in range(len(sentence)-n+1)]\n",
    "        for ngram in temp_ngrams:\n",
    "            if ngram in frequencies:\n",
    "                frequencies[ngram] += 1\n",
    "            else:\n",
    "                frequencies[ngram] = 1\n",
    "    \n",
    "    return frequencies\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method sorts the dictionary items in order of values first, but showing both values and keys. This was used mainly to check my work and test, but I saw it fit to keep it here, as it was something I used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dict_showing_items(unsorteddict):\n",
    "    return dict(sorted(unsorteddict.items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Markov???"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vanilla Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_vanilla(dataset, n):\n",
    "    vanilla = {}\n",
    "    n_grams = calculate_n_gram(dataset, n)\n",
    "    total_grams = sum(n_grams.values())\n",
    "    for key, value in n_grams.items():\n",
    "        vanilla.update({key: value/total_grams})\n",
    "    return vanilla"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Laplace Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The smooth method creats every n-gram that could be possible for a value n where n < 3. I coded different options for n == 1, n == 2 and n == 3, as opposed to recusrively creating for loops to make it work for any value of n since my program will only be used for n-grams up to trigrams. <br>\n",
    "For n == 1, all possible words in the corpus can are permitted. <br>\n",
    "For n == 2, the first word cannot be an end-of-sentence tag and the second word cannot be a start-of-sentence tag. <br>\n",
    "For n == 3, the first word of the trigram can't be an end-of-sentence tag, the second word of the trigram can't be a start- or end-of-sentence tag and the third word of the trigram can't be a start-of-sentence tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(dataset, n):\n",
    "    #creating my vocabulary\n",
    "    vocabulary = []\n",
    "    for sentence in dataset:\n",
    "           for word in sentence:\n",
    "                if word not in vocabulary:\n",
    "                    vocabulary.append(word)\n",
    "\n",
    "    all__possible_ngrams = {}\n",
    "\n",
    "    #if unigram\n",
    "    if n == 1:\n",
    "      \n",
    "      all__possible_ngrams = {x: 1 for x in vocabulary}\n",
    "\n",
    "    if n==2:\n",
    "        for x in vocabulary:\n",
    "            for y in vocabulary:\n",
    "                all__possible_ngrams.update({(x,y): 1})             \n",
    "\n",
    "        #Taking out multiple sentence start and end tags in a row, since this would make no sense.\n",
    "        all__possible_ngrams.update({('<s>', '<s>'): 0})\n",
    "        all__possible_ngrams.update({('</s>', '</s>'): 0})\n",
    "\n",
    "    if n == 3:\n",
    "        for x in vocabulary:\n",
    "            #1st element of the trigram can't be an end of sentence tag\n",
    "            if x == '</s>':\n",
    "                continue\n",
    "            for y in vocabulary:\n",
    "                # 2nd element of the trigram can't be a start or end of sentence tag\n",
    "                if y == '<s>' or y == '</s>':\n",
    "                    continue\n",
    "                for z in vocabulary:\n",
    "                    # 3nd element of the trigram can't be a start of sentence tag\n",
    "                    if z == '<s>':\n",
    "                        continue\n",
    "                    \n",
    "                    all__possible_ngrams.update({(x, y, z): 1})\n",
    "\n",
    "        # Taking out multiple sentence start and end tags in a row, since this would make no sense.\n",
    "    return all__possible_ngrams\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'calculate_laplace' function takes the training set and n as parameters. <br> It first passes these parameters to the smooth method to generate the laplace smoothing effect. It then passes the resultant dictionary as well as the function parameter to the 'calculate_n_grams' function to generate frequencies for all of the corupus' n-grams. <br>\n",
    "The frequencies were then divided by the total n-grams in the corpus to tranform the frequencies into probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_laplace(dataset, n):\n",
    "    laplace = smooth(dataset, n)\n",
    "    laplace = calculate_n_gram(dataset, n, laplace)\n",
    "    total_grams = sum(laplace.values())\n",
    "    for key, value in laplace.items():\n",
    "        laplace.update({key: value/total_grams})\n",
    "    return laplace\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UNK and Laplace Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement the UNK model, I iterated through every n-gram and for each one, if the value is less than or equal to 2, I added it to a counter of UNKs. If there are more than 2 occurences of that n-gram, Laplace smoothing is worked out and the result is added to the final dictionary. Finally, Laplace smoothing is calculated on the UNK counter and it is also added to the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_unk(dataset, n):\n",
    "    unk = {}\n",
    "    unks = 0\n",
    "    n_grams = calculate_n_gram(dataset, n)\n",
    "    total_grams = sum(n_grams.values())\n",
    "    vocabulary = len(n_grams)\n",
    "    for key, value in n_grams.items():\n",
    "        if value <= 2:\n",
    "            unks += value\n",
    "        else:\n",
    "            unk.update({key: value+1/total_grams+vocabulary})\n",
    "            \n",
    "        unk.update({'<UNK>': unks+1/total_grams+vocabulary})\n",
    "    return unk\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Interpolation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Interpolation is calculated by going through a given sentence and calaculating the probabilities for every unigram, bigram and trigram in the sentence for a given flavour of language model (Vanilla, Laplace or UNK). The user chooses which flavour and sentence they would like to use and these, along with the previously-computed dictionary of probabilities are passed as a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_interpolation(input_sentence, language_models_data, flavour):\n",
    "    \n",
    "    trigram_lambda = 0.6\n",
    "    bigram_lambda = 0.3\n",
    "    unigram_lambda = 0.1    \n",
    "\n",
    "    #Cleaning the input (making sure LM flavour names were in the same format they are in in the dictionary)\n",
    "    if flavour == 'Vanilla' or 'VANILLA':\n",
    "        flavour = 'vanilla'\n",
    "\n",
    "    if flavour == 'Laplace' or 'LAPLACE':\n",
    "        flavour = 'laplace'\n",
    "\n",
    "    if flavour == 'unk':\n",
    "        flavour = 'UNK'\n",
    "\n",
    "    probability = 1\n",
    "\n",
    "    tokenised_sentence = (\"<s> \" +input_sentence+ \" </s>\").split()\n",
    "\n",
    "    #len(tokenised_sentence) - 2 was chosen as in a sentence of len x, there are x - 2 trigrams\n",
    "    for i in range(len(tokenised_sentence)-2):\n",
    "        \n",
    "        #accessing the previously-computed probabilities stored in the dictionary\n",
    "        prob_trigram = language_models_data[flavour][(tokenised_sentence[i], tokenised_sentence[i+1], tokenised_sentence[i+2])]\n",
    "        prob_bigram = language_models_data[flavour][(tokenised_sentence[i+1], tokenised_sentence[i+2])]\n",
    "        prob_unigram = language_models_data[flavour][(tokenised_sentence[i+2])]\n",
    "\n",
    "        #multiplying the probability by the given lambda\n",
    "        probability *= unigram_lambda*prob_unigram + bigram_lambda*prob_bigram + trigram_lambda*prob_trigram\n",
    "        \n",
    "    return probability\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the Functions Above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set, test_set = extract(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_language_models = {'vanilla': {\n",
    "                                    'unigram': calculate_vanilla(training_set, 1),\n",
    "                                    'bigram': calculate_vanilla(training_set, 2),\n",
    "                                    'trigram': calculate_vanilla(training_set, 3)\n",
    "                        },\n",
    "\n",
    "                        'laplace': {\n",
    "                                    'unigram': calculate_laplace(training_set, 1),\n",
    "                                    'bigram': calculate_laplace(training_set, 2),\n",
    "                                    'trigram': calculate_laplace(training_set, 3)\n",
    "                        },\n",
    "\n",
    "                        'UNK': {\n",
    "                                'unigram': calculate_unk(training_set, 1),\n",
    "                                'bigram': calculate_unk(training_set, 2),\n",
    "                                'trigram': calculate_unk(training_set, 3)\n",
    "                        }\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity():\n",
    "    pass\n",
    "\n",
    "def calculate_probability(sentence, n, language_dict, flavour):\n",
    "    # Cleaning the input (making sure LM flavour names were in the same format they are in in the dictionary)\n",
    "    if flavour == 'Vanilla' or 'VANILLA':\n",
    "        flavour = 'vanilla'\n",
    "\n",
    "    if flavour == 'Laplace' or 'LAPLACE':\n",
    "        flavour = 'laplace'\n",
    "\n",
    "    if flavour == 'unk':\n",
    "        flavour = 'UNK'\n",
    "\n",
    "    if n == 1:\n",
    "        gram = 'unigram'\n",
    "\n",
    "    if n == 2:\n",
    "        gram = 'bigram'\n",
    "    \n",
    "    if n == 3:\n",
    "        gram = 'trigram'\n",
    "\n",
    "    probability = 1\n",
    "    tokenised_sentence = (\"<s> \" + sentence + \" </s>\").split()\n",
    "\n",
    "    temp_ngrams = [tuple(tokenised_sentence[x:x+n]) for x in range(len(tokenised_sentence)-n+1)]\n",
    "\n",
    "    for x in temp_ngrams:\n",
    "        probability *= language_dict[flavour][gram][x]\n",
    "\n",
    "    return probability"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
