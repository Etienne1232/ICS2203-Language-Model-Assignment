{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICS2203 â€“ Statistical Natural Language Processing\n",
    "## Building a Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting data from the corpus."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading raw data from the Download files provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_data(file_directory):\n",
    "    all_data = []\n",
    "\n",
    "    #appends all files to all_data\n",
    "    for x in os.listdir(file_directory):\n",
    "        for y in os.listdir(file_directory + x):\n",
    "            all_data.append(ET.parse(file_directory+x+'/'+y))\n",
    "    \n",
    "    return all_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating through every xml file in the dataset and extracts all text from the files. <br>\n",
    "The data is saved in a 2D list. Every sub-list in the main list will contain 1 sentence. Every element in the sublist will contain 1 word or punctuation mark. <br>\n",
    "Some words ended with a space in the corpus, so I made sure to negate that space so that, for example: 'hello' and 'hello ' count as the same word. <br>\n",
    "I added start- and end-of-sentence tags to ease computation later on. <br> The dataset was then shuffled and a random 15% of the dataset was taken to be the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(data):\n",
    "    dataset = []\n",
    "    for file in data:\n",
    "        for tree in file.getroot():\n",
    "            for sentence in tree.iter('s'):\n",
    "                temp_sentence = ['<s>']\n",
    "                for word in sentence:\n",
    "                        if word.text is not None:\n",
    "                            if word.text[-1] == ' ':\n",
    "                                temp_sentence.append(word.text[:-1])\n",
    "                            else:\n",
    "                                temp_sentence.append(word.text)\n",
    "                temp_sentence.append(\"</s>\")\n",
    "                dataset.append(temp_sentence)\n",
    "\n",
    "    random.shuffle(dataset)\n",
    "    slicing_point = int(len(dataset) * 0.85)\n",
    "    return dataset[:slicing_point], dataset[slicing_point:] "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating N-Grams"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function takes a dataset, the n (amount of elements in the n-gram), and if, provided, it takes a dictionary with already defined words and frequencies. <br>\n",
    "This method iterates through every sentence in the training dataset and generates n-grams for a provided n. <br>\n",
    "These n-grams are then tallied into a dictionary, where every n-gram is saved as a key, and the amount of times it appears is its value. <br>\n",
    "This dictionary is then returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_n_gram(data, n, frequencies = {}):\n",
    "    for sentence in data:\n",
    "        #len(sentence)-n+1 was taken as for each n-gram, there are that many windows of words(grams)\n",
    "        temp_ngrams = [tuple(sentence[x:x+n]) for x in range(len(sentence)-n+1)]\n",
    "        for ngram in temp_ngrams:\n",
    "            if ngram in frequencies:\n",
    "                frequencies[ngram] += 1\n",
    "            else:\n",
    "                frequencies[ngram] = 1\n",
    "    \n",
    "    return frequencies\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method sorts the dictionary items in order of values first, but showing both values and keys. This was used mainly to check my work and test, but I saw it fit to keep it here, as it was something I used."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vanilla Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_vanilla(dataset, n):\n",
    "    vanilla = {}\n",
    "    n_grams = calculate_n_gram(dataset, n)\n",
    "    total_grams = sum(n_grams.values())\n",
    "    for key, value in n_grams.items():\n",
    "        vanilla.update({key: value/total_grams})\n",
    "    return vanilla"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Laplace Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The smooth method creats every n-gram that could be possible for a value n where n < 3. I coded different options for n == 1, n == 2 and n == 3, as opposed to recusrively creating for loops to make it work for any value of n since my program will only be used for n-grams up to trigrams. <br>\n",
    "For n == 1, all possible words in the corpus can are permitted. <br>\n",
    "For n == 2, the first word cannot be an end-of-sentence tag and the second word cannot be a start-of-sentence tag. <br>\n",
    "For n == 3, the first word of the trigram can't be an end-of-sentence tag, the second word of the trigram can't be a start- or end-of-sentence tag and the third word of the trigram can't be a start-of-sentence tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(dataset, n):\n",
    "    #creating my vocabulary\n",
    "    vocabulary = []\n",
    "    for sentence in dataset:\n",
    "           for word in sentence:\n",
    "                if word not in vocabulary:\n",
    "                    vocabulary.append(word)\n",
    "\n",
    "    all__possible_ngrams = {}\n",
    "\n",
    "    #if unigram\n",
    "    if n == 1:\n",
    "      \n",
    "      all__possible_ngrams = {x: 1 for x in vocabulary}\n",
    "\n",
    "    if n==2:\n",
    "        for x in vocabulary:\n",
    "            for y in vocabulary:\n",
    "                all__possible_ngrams.update({(x,y): 1})             \n",
    "\n",
    "        #Taking out multiple sentence start and end tags in a row, since this would make no sense.\n",
    "        all__possible_ngrams.update({('<s>', '<s>'): 0})\n",
    "        all__possible_ngrams.update({('</s>', '</s>'): 0})\n",
    "\n",
    "    if n == 3:\n",
    "        for x in vocabulary:\n",
    "            #1st element of the trigram can't be an end of sentence tag\n",
    "            if x == '</s>':\n",
    "                continue\n",
    "            for y in vocabulary:\n",
    "                # 2nd element of the trigram can't be a start or end of sentence tag\n",
    "                if y == '<s>' or y == '</s>':\n",
    "                    continue\n",
    "                for z in vocabulary:\n",
    "                    # 3nd element of the trigram can't be a start of sentence tag\n",
    "                    if z == '<s>':\n",
    "                        continue\n",
    "                    \n",
    "                    all__possible_ngrams.update({(x, y, z): 1})\n",
    "\n",
    "        # Taking out multiple sentence start and end tags in a row, since this would make no sense.\n",
    "    return all__possible_ngrams\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'calculate_laplace' function takes the training set and n as parameters. <br> It first passes these parameters to the smooth method to generate the laplace smoothing effect. It then passes the resultant dictionary as well as the function parameter to the 'calculate_n_grams' function to generate frequencies for all of the corupus' n-grams. <br>\n",
    "The frequencies were then divided by the total n-grams in the corpus to tranform the frequencies into probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_laplace(dataset, n):\n",
    "    laplace = smooth(dataset, n)\n",
    "    laplace = calculate_n_gram(dataset, n, laplace)\n",
    "    total_grams = sum(laplace.values())\n",
    "    for key, value in laplace.items():\n",
    "        laplace.update({key: value/total_grams})\n",
    "    return laplace\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UNK and Laplace Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement the UNK model, I iterated through every n-gram and for each one, if the value is less than or equal to 2, I added it to a counter of UNKs. If there are more than 2 occurences of that n-gram, Laplace smoothing is worked out and the result is added to the final dictionary. Finally, Laplace smoothing is calculated on the UNK counter and it is also added to the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_unk(dataset, n):\n",
    "    unk = {}\n",
    "    unks = 0\n",
    "    n_grams = calculate_n_gram(dataset, n)\n",
    "    total_grams = sum(n_grams.values())\n",
    "    vocabulary = len(n_grams)\n",
    "    for key, value in n_grams.items():\n",
    "        if value <= 2:\n",
    "            unks += value\n",
    "        else:\n",
    "            unk.update({key: value+1/total_grams+vocabulary})\n",
    "            \n",
    "        unk.update({'<UNK>': unks+1/total_grams+vocabulary})\n",
    "    return unk\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Interpolation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Interpolation is calculated by going through a given sentence and calaculating the probabilities for every unigram, bigram and trigram in the sentence for a given flavour of language model (Vanilla, Laplace or UNK). The user chooses which flavour and sentence they would like to use and these, along with the previously-computed dictionary of probabilities are passed as a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_interpolation_sentence(input_sentence, language_models_data, flavour):\n",
    "    \n",
    "    trigram_lambda = 0.6\n",
    "    bigram_lambda = 0.3\n",
    "    unigram_lambda = 0.1    \n",
    "\n",
    "    #Cleaning the input (making sure LM flavour names were in the same format they are in in the dictionary)\n",
    "    if flavour == 'Vanilla' or 'VANILLA':\n",
    "        flavour = 'vanilla'\n",
    "\n",
    "    if flavour == 'Laplace' or 'LAPLACE':\n",
    "        flavour = 'laplace'\n",
    "\n",
    "    if flavour == 'unk':\n",
    "        flavour = 'UNK'\n",
    "\n",
    "    probability = 1\n",
    "\n",
    "    tokenised_sentence = (\"<s> \" +input_sentence+ \" </s>\").split()\n",
    "\n",
    "    #len(tokenised_sentence) - 2 was chosen as in a sentence of len x, there are x - 2 trigrams\n",
    "    for i in range(len(tokenised_sentence)-2):\n",
    "        \n",
    "        #accessing the previously-computed probabilities stored in the dictionary\n",
    "        prob_trigram = language_models_data[flavour][(tokenised_sentence[i], tokenised_sentence[i+1], tokenised_sentence[i+2])]\n",
    "        prob_bigram = language_models_data[flavour][(tokenised_sentence[i+1], tokenised_sentence[i+2])]\n",
    "        prob_unigram = language_models_data[flavour][(tokenised_sentence[i+2])]\n",
    "\n",
    "        #multiplying the probability by the given lambda\n",
    "        probability *= unigram_lambda*prob_unigram + bigram_lambda*prob_bigram + trigram_lambda*prob_trigram\n",
    "        \n",
    "    return probability, len(tokenised_sentence)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method extends the above's usage from 1 sentence to a whole dataset, by multiplying the probability values returned by the above method for each sentence in the dataset. It also returns the perplexity instead of just the probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_interpolation_perplexity(dataset, language_models_data, flavour):\n",
    "    value = 1\n",
    "    len = 0\n",
    "    for sentence in dataset:\n",
    "        temp_value, temp_len = linear_interpolation_sentence(sentence, language_models_data, flavour)\n",
    "        value *= temp_value\n",
    "        len += temp_len\n",
    "    return pow(value, -1/(len))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_probability(sentence, n, language_dict, flavour):\n",
    "    # Cleaning the input (making sure LM flavour names were in the same format they are in in the dictionary)\n",
    "    if flavour == 'Vanilla' or 'VANILLA':\n",
    "        flavour = 'vanilla'\n",
    "\n",
    "    if flavour == 'Laplace' or 'LAPLACE':\n",
    "        flavour = 'laplace'\n",
    "\n",
    "    if flavour == 'unk':\n",
    "        flavour = 'UNK'\n",
    "\n",
    "    if n == 1:\n",
    "        gram = 'unigram'\n",
    "\n",
    "    elif n == 2:\n",
    "        gram = 'bigram'\n",
    "\n",
    "    else: #ie. n == 3\n",
    "        gram = 'trigram'\n",
    "\n",
    "    probability = 1\n",
    "    tokenised_sentence = (\"<s> \" + sentence + \" </s>\").split()\n",
    "\n",
    "    temp_ngrams = [tuple(tokenised_sentence[x:x+n]) for x in range(len(tokenised_sentence)-n+1)]\n",
    "\n",
    "    for x in temp_ngrams:\n",
    "        probability *= language_dict[flavour][gram][x]\n",
    "\n",
    "    return probability, len(tokenised_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(test_data, n, language_dict, flavour):\n",
    "    probability = 1\n",
    "    length = 0\n",
    "    for sentence in test_data:\n",
    "        temp_prob, temp_len = calculate_probability(sentence, n, language_dict, flavour)\n",
    "        probability *= temp_prob\n",
    "        length += temp_len\n",
    "\n",
    "    return pow(probability, -1/(length))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the Functions Needed to Build the Language Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "all_data = raw_data('./Data/Texts/')\n",
    "training_set, test_set = extract(all_data)\n",
    "all_language_models = {\n",
    "    'vanilla': {\n",
    "        'unigram': calculate_vanilla(training_set, 1),\n",
    "        'bigram': calculate_vanilla(training_set, 2),\n",
    "        'trigram': calculate_vanilla(training_set, 3)\n",
    "    },\n",
    "\n",
    "    'laplace': {\n",
    "        'unigram': calculate_laplace(training_set, 1),\n",
    "        'bigram': calculate_laplace(training_set, 2),\n",
    "        'trigram': calculate_laplace(training_set, 3)\n",
    "    },\n",
    "\n",
    "    'UNK': {\n",
    "        'unigram': calculate_unk(training_set, 1),\n",
    "        'bigram': calculate_unk(training_set, 2),\n",
    "        'trigram': calculate_unk(training_set, 3)\n",
    "    }\n",
    "}\n",
    "end = time.time()\n",
    "\n",
    "time_taken = start-end\n",
    "print(time_taken)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locally Saving the Language Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_data(data):\n",
    "    with open(\"datadump.json\", \"w\") as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "def open_data():\n",
    "    with open(\"datadump.json\", \"r\") as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculating and Printing the Perplexity Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\".center(20), \"Unigram\".center(20), 'Bigram'.center(20),'Trigram'.center(20), 'Linear Interpolation'.center(20))\n",
    "print(\"Vanilla\".center(20), str(calculate_perplexity(test_set, 1, all_language_models, 'vanilla'))[0:20], str(calculate_perplexity(test_set, 2, all_language_models, 'vanilla'))[0:20], str(calculate_perplexity(test_set, 3, all_language_models, 'vanilla'))[0:20], str(linear_interpolation_perplexity(test_set, all_language_models, 'vanilla'))[0:20])\n",
    "print(\"Laplace\".center(20), str(calculate_perplexity(test_set, 1, all_language_models, 'laplace'))[0:20], calculate_perplexity(test_set, 2, all_language_models, 'laplace')[0:20], calculate_perplexity(test_set, 3, all_language_models, 'laplace')[0:20], str(linear_interpolation_perplexity(test_set, all_language_models, 'laplace'))[0:20])\n",
    "print(\"UNK\".center(20), str(calculate_perplexity(test_set, 1, all_language_models, 'UNK'))[0:20], str(calculate_perplexity(test_set, 2, all_language_models, 'UNK'))[0:20], str(calculate_perplexity(test_set, 3, all_language_models, 'UNK'))[0:20], str(linear_interpolation_perplexity(test_set, all_language_models, 'UNK'))[0:20])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(language_dict, flavour):\n",
    "    if flavour == 'Vanilla' or 'VANILLA':\n",
    "        flavour = 'vanilla'\n",
    "\n",
    "    if flavour == 'Laplace' or 'LAPLACE':\n",
    "        flavour = 'laplace'\n",
    "\n",
    "    if flavour == 'unk':\n",
    "        flavour = 'UNK'\n",
    "    \n",
    "    generated = ''\n",
    "\n",
    "    #Unigram\n",
    "    while generated != '</s>':\n",
    "        language_dict[flavour]['unigram']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dict_showing_items(unsorteddict):\n",
    "    return dict(sorted(unsorteddict.items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation - Responding to the questions on the assignment brief."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a. How large is the corpus that you are working with? What splits did you use?** <br>\n",
    "> I  used the whole (Baby) British National Corpus. It is XXXXX words long. I took a random 85% of the data as a training set and the other 15% as a test set. <br>\n",
    "\n",
    "**b. How much time does it take to build the language models?** <br>\n",
    "> {{time_taken}} seconds\n",
    "\n",
    "**c. How much space did your datastructures require once all models were learnt?** <br>\n",
    "> TBD sys.getsizeof() Bytes\n",
    "\n",
    "**d. Take a test sentence and output its probabiliy using all the different models â€“ discuss this output.** <br>\n",
    "\n",
    "**e. Take a test input and generate a sentence using the different models â€“ discuss the output.** <br>\n",
    "\n",
    "**f. The testing part (15 marks) should detail how you tested the different compontents throughout your project development, what results were obtained and what fixes, if any, were required.** <br>\n",
    "\n",
    "**g. Specify any additional features that youâ€™d like to point out.** <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
