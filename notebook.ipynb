{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmJW0xvtcKpo"
   },
   "source": [
    "# ICS2203 â€“ Statistical Natural Language Processing\n",
    "## Building a Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHUVkO1McKpu"
   },
   "source": [
    "### Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Goa6hqu8cKpu"
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TlXoIpgMcKpw"
   },
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1dUVTbjPcKpw"
   },
   "source": [
    "#### Extracting data from the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BAahN2kicKpw"
   },
   "source": [
    "Reading raw data from the Download files provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "AwkErxAbcKpx"
   },
   "outputs": [],
   "source": [
    "def raw_data(file_directory):\n",
    "    all_data = []\n",
    "\n",
    "    #appends all files to all_data\n",
    "    for x in os.listdir(file_directory):\n",
    "        for y in os.listdir(file_directory + x):\n",
    "            all_data.append(ET.parse(file_directory+x+'/'+y))\n",
    "    \n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrnD3zTHcKpx"
   },
   "source": [
    "Iterating through every xml file in the dataset and extracts all text from the files. <br>\n",
    "The data is saved in a 2D list. Every sub-list in the main list will contain 1 sentence. Every element in the sublist will contain 1 word or punctuation mark. <br>\n",
    "Some words ended with a space in the corpus, so I made sure to negate that space so that, for example: 'hello' and 'hello ' count as the same word. <br>\n",
    "I added start- and end-of-sentence tags to ease computation later on. <br> The dataset was then shuffled and a random 15% of the dataset was taken to be the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "up1toPKTcKpx"
   },
   "outputs": [],
   "source": [
    "def extract(data):\n",
    "    dataset = []\n",
    "    for file in data:\n",
    "        for tree in file.getroot():\n",
    "            for sentence in tree.iter('s'):\n",
    "                temp_sentence = ['<s>']\n",
    "                for word in sentence:\n",
    "                        if word.text is not None:\n",
    "                            if word.text[-1] == ' ':\n",
    "                                temp_sentence.append(word.text[:-1])\n",
    "                            else:\n",
    "                                temp_sentence.append(word.text)\n",
    "                temp_sentence.append(\"</s>\")\n",
    "                dataset.append(temp_sentence)\n",
    "    random.shuffle(dataset)\n",
    "    slicing_point = int(len(dataset) * 0.85)\n",
    "    return dataset[:slicing_point], dataset[slicing_point:] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Try7HW9hcKpy"
   },
   "source": [
    "#### Calculating N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xe8-CMkKcKpy"
   },
   "source": [
    "The following function takes a dataset, the n (amount of elements in the n-gram), and if, provided, it takes a dictionary with already defined words and frequencies. <br>\n",
    "This method iterates through every sentence in the training dataset and generates n-grams for a provided n. <br>\n",
    "These n-grams are then tallied into a dictionary, where every n-gram is saved as a key, and the amount of times it appears is its value. <br>\n",
    "This dictionary is then returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "o2eNY_68cKpy"
   },
   "outputs": [],
   "source": [
    "def calculate_n_gram(dataIn, n, frequencies = None, unk = False):\n",
    "    #instead of setting frequencies = {} in the parameters, I used this, as if I did the previously mentioned, it would be created at the first function call and stay in place\n",
    "    if frequencies is None:\n",
    "        frequencies = {}\n",
    "    unk_list = set()\n",
    "    #if we are using this for UNK\n",
    "    if unk:\n",
    "        number_of_unk = 0\n",
    "        #it recursively calls itself and calculates all unigrams in the set\n",
    "        word_counts = calculate_n_gram(dataIn, 1)\n",
    "        #checking which word frequencies are 2 or less\n",
    "        for key, value in word_counts.items():\n",
    "            #in the case they are: \n",
    "            if value <= 2:\n",
    "                # the counter is incremented by the number of times the word appears\n",
    "                number_of_unk += value\n",
    "                #the word is extracted from the tuple and added to the list which will be transformed into <UNK> tokens\n",
    "                unk_list.add(key[0]) \n",
    "    \n",
    "    ngrams = []\n",
    "    for sentence in dataIn:\n",
    "        #len(sentence)-n+1 was taken as for each n-gram, there are that many windows of words(grams)\n",
    "        temp_ngrams = [tuple(sentence[x:x+n]) for x in range(len(sentence)-n+1)]\n",
    "        \n",
    "        if unk:\n",
    "            if n == 1:\n",
    "                temp_ngrams = [('<UNK>',) if x in unk_list else (x,) for x in temp_ngrams]\n",
    "\n",
    "            elif n == 2:\n",
    "                temp_ngrams = [(('<UNK>', y[1]) if y[0] in unk_list else (y[0], '<UNK>')) if y[0] in unk_list or y[1] in unk_list else y for y in temp_ngrams]\n",
    "\n",
    "            elif n == 3: \n",
    "                temp_ngrams = [(('<UNK>', y[1], y[2]) if y[0] in unk_list else (y[0], '<UNK>', y[2]) if y[1] in unk_list else (y[0], y[1], '<UNK>') if y[2] in unk_list else y) for y in temp_ngrams]\n",
    "        \n",
    "        for ngram in temp_ngrams:\n",
    "            if ngram in frequencies:\n",
    "                frequencies[ngram] += 1\n",
    "            else:\n",
    "                frequencies[ngram] = 1\n",
    "    return frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-bwIbwH5cKpz"
   },
   "source": [
    "This method sorts the dictionary items in order of values first, but showing both values and keys. This was used mainly to check my work and test, but I saw it fit to keep it here, as it was something I used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7woqRS-ycKpz"
   },
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eDDtGph3cKpz"
   },
   "source": [
    "#### Vanilla Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "NlVC01JtcKpz"
   },
   "outputs": [],
   "source": [
    "def calculate_vanilla(dataset, n):\n",
    "    vanilla = {}\n",
    "    n_grams = calculate_n_gram(dataset, n)\n",
    "    total_grams = sum(n_grams.values())\n",
    "    for key, value in n_grams.items():\n",
    "        vanilla.update({key: value/total_grams})\n",
    "    del n_grams\n",
    "    return vanilla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r48sDkfwcKpz"
   },
   "source": [
    "#### Laplace Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "La6I1uYhcKpz"
   },
   "source": [
    "The smooth method creats every n-gram that could be possible for a value n where n < 3. I coded different options for n == 1, n == 2 and n == 3, as opposed to recusrively creating for loops to make it work for any value of n since my program will only be used for n-grams up to trigrams. <br>\n",
    "For n == 1, all possible words in the corpus can are permitted. <br>\n",
    "For n == 2, the first word cannot be an end-of-sentence tag and the second word cannot be a start-of-sentence tag. <br>\n",
    "For n == 3, the first word of the trigram can't be an end-of-sentence tag, the second word of the trigram can't be a start- or end-of-sentence tag and the third word of the trigram can't be a start-of-sentence tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "NS1UvW0lcKpz"
   },
   "outputs": [],
   "source": [
    "def smooth(dataset, n):\n",
    "    #creating my vocabulary\n",
    "    vocabulary = []\n",
    "    for sentence in dataset:\n",
    "           for word in sentence:\n",
    "                if word not in vocabulary:\n",
    "                    vocabulary.append(word)\n",
    "\n",
    "    all__possible_ngrams = {}\n",
    "\n",
    "    #if unigram\n",
    "    if n == 1:\n",
    "      \n",
    "      all__possible_ngrams = {x: 1 for x in vocabulary}\n",
    "\n",
    "    if n==2:\n",
    "        for x in vocabulary:\n",
    "            for y in vocabulary:\n",
    "                all__possible_ngrams.update({(x,y): 1})             \n",
    "\n",
    "        #Taking out multiple sentence start and end tags in a row, since this would make no sense.\n",
    "        all__possible_ngrams.update({('<s>', '<s>'): 0})\n",
    "        all__possible_ngrams.update({('</s>', '</s>'): 0})\n",
    "\n",
    "    if n == 3:\n",
    "        for x in vocabulary:\n",
    "            #1st element of the trigram can't be an end of sentence tag\n",
    "            if x == '</s>':\n",
    "                continue\n",
    "            for y in vocabulary:\n",
    "                # 2nd element of the trigram can't be a start or end of sentence tag\n",
    "                if y == '<s>' or y == '</s>':\n",
    "                    continue\n",
    "                for z in vocabulary:\n",
    "                    # 3nd element of the trigram can't be a start of sentence tag\n",
    "                    if z == '<s>':\n",
    "                        continue\n",
    "                    \n",
    "                    all__possible_ngrams.update({(x, y, z): 1})\n",
    "    del vocabulary\n",
    "    return all__possible_ngrams\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8qnhS2vcKp0"
   },
   "source": [
    "The 'calculate_laplace' function takes the training set and n as parameters. <br> It first passes these parameters to the smooth method to generate the laplace smoothing effect. It then passes the resultant dictionary as well as the function parameter to the 'calculate_n_grams' function to generate frequencies for all of the corupus' n-grams. <br>\n",
    "The frequencies were then divided by the total n-grams in the corpus to tranform the frequencies into probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "gj1689KAcKp0"
   },
   "outputs": [],
   "source": [
    "def calculate_laplace(dataset, n):\n",
    "    laplace = smooth(dataset, n)\n",
    "    laplace = calculate_n_gram(dataset, n, laplace)\n",
    "    answer = {}\n",
    "    total_grams = sum(laplace.values())\n",
    "    for key, value in laplace.items():\n",
    "        answer.update({key: value/total_grams})\n",
    "    del laplace\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GslO8SPKcKp0"
   },
   "source": [
    "#### UNK and Laplace Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GKGNnGRqcKp0"
   },
   "source": [
    "To implement the UNK model, I iterated through every n-gram and for each one, if the value is less than or equal to 2, I added it to a counter of UNKs. If there are more than 2 occurences of that n-gram, Laplace smoothing is worked out and the result is added to the final dictionary. Finally, Laplace smoothing is calculated on the UNK counter and it is also added to the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "RrpPVepTcKp0"
   },
   "outputs": [],
   "source": [
    "def calculate_unk(dataIn, n):\n",
    "    unk = {}\n",
    "    n_grams = calculate_n_gram(dataIn, n, unk=True)\n",
    "    total_grams = sum(n_grams.values())\n",
    "    vocabulary_size = len(n_grams)\n",
    "    for key, value in n_grams.items():\n",
    "        unk.update({key: (value+1)/(total_grams+vocabulary_size)})\n",
    "    del n_grams\n",
    "    return unk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5HfOrhmcKp1"
   },
   "source": [
    "#### Linear Interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h8TFuJvlcKp1"
   },
   "source": [
    "Linear Interpolation is calculated by going through a given sentence and calaculating the probabilities for every unigram, bigram and trigram in the sentence for a given flavour of language model (Vanilla, Laplace or UNK). The user chooses which flavour and sentence they would like to use and these, along with the previously-computed dictionary of probabilities are passed as a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Y_OFNVz-cKp1"
   },
   "outputs": [],
   "source": [
    "def linear_interpolation_sentence(input_sentence, language_models_data, flavour):\n",
    "    \n",
    "    trigram_lambda = 0.6\n",
    "    bigram_lambda = 0.3\n",
    "    unigram_lambda = 0.1    \n",
    "\n",
    "    #Cleaning the input (making sure LM flavour names were in the same format they are in in the dictionary)\n",
    "    if flavour == 'Vanilla' or 'VANILLA':\n",
    "        flavour = 'vanilla'\n",
    "\n",
    "    if flavour == 'Laplace' or 'LAPLACE':\n",
    "        flavour = 'laplace'\n",
    "\n",
    "    if flavour == 'unk':\n",
    "        flavour = 'UNK'\n",
    "\n",
    "    probability = 1\n",
    "\n",
    "    tokenised_sentence = (\"<s> \" +input_sentence+ \" </s>\").split()\n",
    "\n",
    "    #len(tokenised_sentence) - 2 was chosen as in a sentence of len x, there are x - 2 trigrams\n",
    "    for i in range(len(tokenised_sentence)-2):\n",
    "        \n",
    "        #accessing the previously-computed probabilities stored in the dictionary\n",
    "        prob_trigram = language_models_data[flavour][(tokenised_sentence[i], tokenised_sentence[i+1], tokenised_sentence[i+2])]\n",
    "        prob_bigram = language_models_data[flavour][(tokenised_sentence[i+1], tokenised_sentence[i+2])]\n",
    "        prob_unigram = language_models_data[flavour][(tokenised_sentence[i+2])]\n",
    "\n",
    "        #multiplying the probability by the given lambda\n",
    "        probability *= unigram_lambda*prob_unigram + bigram_lambda*prob_bigram + trigram_lambda*prob_trigram\n",
    "        \n",
    "    return probability, len(tokenised_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3AvJgejrcKp1"
   },
   "source": [
    "The following method extends the above's usage from 1 sentence to a whole dataset, by multiplying the probability values returned by the above method for each sentence in the dataset. It also returns the perplexity instead of just the probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "HPGN6A70cKp1"
   },
   "outputs": [],
   "source": [
    "def linear_interpolation_perplexity(dataset, language_models_data, flavour):\n",
    "    value = 1\n",
    "    len = 0\n",
    "    for sentence in dataset:\n",
    "        temp_value, temp_len = linear_interpolation_sentence(sentence, language_models_data, flavour)\n",
    "        value *= temp_value\n",
    "        len += temp_len\n",
    "    return pow(value, -1/(len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pd0Fil0AcKp1"
   },
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "5bvCyAcrcKp2"
   },
   "outputs": [],
   "source": [
    "def calculate_probability(sentence, n, language_dict, flavour):\n",
    "    # Cleaning the input (making sure LM flavour names were in the same format they are in in the dictionary)\n",
    "    if flavour == 'Vanilla' or 'VANILLA':\n",
    "        flavour = 'vanilla'\n",
    "\n",
    "    if flavour == 'Laplace' or 'LAPLACE':\n",
    "        flavour = 'laplace'\n",
    "\n",
    "    if flavour == 'unk':\n",
    "        flavour = 'UNK'\n",
    "\n",
    "    if n == 1:\n",
    "        gram = 'unigram'\n",
    "\n",
    "    elif n == 2:\n",
    "        gram = 'bigram'\n",
    "\n",
    "    else: #ie. n == 3\n",
    "        gram = 'trigram'\n",
    "\n",
    "    probability = 1\n",
    "    tokenised_sentence = (\"<s> \" + sentence + \" </s>\").split()\n",
    "\n",
    "    temp_ngrams = [tuple(tokenised_sentence[x:x+n]) for x in range(len(tokenised_sentence)-n+1)]\n",
    "\n",
    "    for x in temp_ngrams:\n",
    "        probability *= language_dict[flavour][gram][x]\n",
    "\n",
    "    return probability, len(tokenised_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "LWi-bKBUcKp2"
   },
   "outputs": [],
   "source": [
    "def calculate_perplexity(test_data, n, language_dict, flavour):\n",
    "    probability = 1\n",
    "    length = 0\n",
    "    for sentence in test_data:\n",
    "        temp_prob, temp_len = calculate_probability(sentence, n, language_dict, flavour)\n",
    "        probability *= temp_prob\n",
    "        length += temp_len\n",
    "\n",
    "    return pow(probability, -1/(length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dySaHb7FcKp2"
   },
   "source": [
    "#### Locally Saving the Language Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "id": "RpPzAuo0cKp2"
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def dump_data(data, name):\n",
    "    data = {str(key): value for key, value in data.items()}\n",
    "    with open('./Language Models/'+name+\".json\", \"w\") as d:\n",
    "        json.dump(data, d)\n",
    "\n",
    "def open_data(name):\n",
    "    with open('./Language Models/'+name+\".json\", \"r\") as o:\n",
    "        return {ast.literal_eval(k): v for k, v in json.load(o).items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0KRrzGUcKp2"
   },
   "source": [
    "#### Building and Dumping the Language Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_and_dump(training_set):\n",
    "    dump_data(calculate_vanilla(training_set, 1), 'vanilla unigram')\n",
    "    dump_data(calculate_vanilla(training_set, 2), 'vanilla bigram')\n",
    "    dump_data(calculate_vanilla(training_set, 3), 'vanilla trigram')\n",
    "    \n",
    "    dump_data(calculate_laplace(training_set, 1), 'laplace unigram')\n",
    "    dump_data(calculate_laplace(training_set, 2), 'laplace bigram')\n",
    "    dump_data(calculate_laplace(training_set, 3), 'laplace trigram')\n",
    "\n",
    "    dump_data(calculate_unk(training_set, 1), 'unk unigram')\n",
    "    dump_data(calculate_unk(training_set, 2), 'unk bigram')\n",
    "    dump_data(calculate_unk(training_set, 3), 'unk trigram')\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "8SBrCN5jcKp2",
    "tags": []
   },
   "source": [
    "start = time.time()\n",
    "all_data = raw_data('./Full Data/Data/Texts/')\n",
    "training_set, test_set = extract(all_data)\n",
    "build_and_dump(training_set)\n",
    "end = time.time()\n",
    "time_taken = end - start\n",
    "print(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dZ4OGGUtcKp2"
   },
   "source": [
    "##### Calculating and Printing the Perplexity Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ggmI04kcKp3"
   },
   "outputs": [],
   "source": [
    "print(\"\".center(20), \"Unigram\".center(20), 'Bigram'.center(20),'Trigram'.center(20), 'Linear Interpolation'.center(20))\n",
    "print(\"Vanilla\".center(20), str(calculate_perplexity(test_set, 1, all_language_models, 'vanilla'))[0:20], str(calculate_perplexity(test_set, 2, all_language_models, 'vanilla'))[0:20], str(calculate_perplexity(test_set, 3, all_language_models, 'vanilla'))[0:20], str(linear_interpolation_perplexity(test_set, all_language_models, 'vanilla'))[0:20])\n",
    "print(\"Laplace\".center(20), str(calculate_perplexity(test_set, 1, all_language_models, 'laplace'))[0:20], calculate_perplexity(test_set, 2, all_language_models, 'laplace')[0:20], calculate_perplexity(test_set, 3, all_language_models, 'laplace')[0:20], str(linear_interpolation_perplexity(test_set, all_language_models, 'laplace'))[0:20])\n",
    "print(\"UNK\".center(20), str(calculate_perplexity(test_set, 1, all_language_models, 'UNK'))[0:20], str(calculate_perplexity(test_set, 2, all_language_models, 'UNK'))[0:20], str(calculate_perplexity(test_set, 3, all_language_models, 'UNK'))[0:20], str(linear_interpolation_perplexity(test_set, all_language_models, 'UNK'))[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HyfOOkC-cKp3"
   },
   "source": [
    "#### Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I exported the tuples to json, and so had to turn them into strings, a more complex system had to be put in to extract the words from the tuples in text form. Example: If the above returned ('hello'), it is in the form of a string, and so indexing would only give '('. To counteract this, I implemented the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose(language_dict, n):\n",
    "    choice = random.choices(list(language_dict.keys()), weights = list(language_dict.values()), k = 1)[0]\n",
    "    return choice[n-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_bi_tr(language_dict, n, list_of_words):\n",
    "    if n == 2:\n",
    "        return {k: v for k, v in language_dict.items() if k[0] == list_of_words[0]}\n",
    "     \n",
    "    if n == 3:\n",
    "        return {k: v for k, v in language_dict.items() if k[0] == list_of_words[0] and k[1] == list_of_words[1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(language_dict, n, input_sentence):\n",
    "    tokenized = input_sentence.split()\n",
    "    generated = input_sentence\n",
    "    temp = None\n",
    "    \n",
    "    # Unigram\n",
    "    if n == 1:\n",
    "        while temp != '</s>':\n",
    "            temp = choose(language_dict)\n",
    "            if temp != '<s>':\n",
    "                generated += \" \" + temp\n",
    "\t# Bigram\n",
    "    if n == 2:\n",
    "        while temp != '</s>':\n",
    "            window = tokenized[-1]\n",
    "            filtered_dict = filter_bi_tr(language_dict, n, [window])\n",
    "            temp = choose(filtered_dict, 2)\n",
    "            if temp != '<s>':\n",
    "                generated += \" \" + temp\n",
    "                window = temp\n",
    "    if n == 3:\n",
    "        while temp != '</s>':\n",
    "            window = tokenized[-2:]\n",
    "            filtered_dict = filter_bi_tr(language_dict, n, window)\n",
    "            temp = choose(filtered_dict, 3)\n",
    "            if temp != '<s>':\n",
    "                generated += \" \" + temp\n",
    "                window = temp\n",
    "                \n",
    "    return generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sen_Probability(sentence, language_dict, n, unk = False):\n",
    "    tokenized = ['<s>'] + input_sentence.split() + ['</s>']\n",
    "    probability = 1\n",
    "    \n",
    "    for i in range(n-1, len(tokenized)):\n",
    "        grams = tuple(tokenized[i-n+1: i+1])\n",
    "    if ngram in language_dict:\n",
    "        probability *= language_dict[ngram]\n",
    "\t\n",
    "    elif unk:\n",
    "        if n == 1:\n",
    "            probability *= language_dict[('<UNK>',)]\n",
    "\t\t\n",
    "        if n == 2:\n",
    "            x = language_dict[('<UNK>', ngram[1])]\n",
    "            y = language_dict[(ngram[1], '<UNK>')]\n",
    "            if x is not None:\n",
    "                probability *= x\n",
    "            elif y is not None:\n",
    "                probability *= y\n",
    "            else:\n",
    "                probability = 0\n",
    "    else:\n",
    "        probability *= 0\n",
    "    \n",
    "    return probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7_cS0QddcKp9"
   },
   "source": [
    "#### Testing Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "id": "bPz0pJ2vcKp-"
   },
   "outputs": [],
   "source": [
    "def sort_dict_showing_items(unsorteddict):\n",
    "    return dict(sorted(unsorteddict.items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BkEQqvgVcKp_"
   },
   "source": [
    "### Documentation - Responding to the questions on the assignment brief."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ab6zsjsNcKqA"
   },
   "source": [
    "**a. How large is the corpus that you are working with? What splits did you use?** <br>\n",
    "> I  used the whole (Baby) British National Corpus. It is XXXXX words long. I took a random 85% of the data as a training set and the other 15% as a test set. <br>\n",
    "\n",
    "**b. How much time does it take to build the language models?** <br>\n",
    "> {{time_taken}} seconds\n",
    "\n",
    "**c. How much space did your datastructures require once all models were learnt?** <br>\n",
    "> TBD sys.getsizeof() Bytes\n",
    "\n",
    "**d. Take a test sentence and output its probabiliy using all the different models â€“ discuss this output.** <br>\n",
    "\n",
    "**e. Take a test input and generate a sentence using the different models â€“ discuss the output.** <br>\n",
    "\n",
    "**f. The testing part (15 marks) should detail how you tested the different compontents throughout your project development, what results were obtained and what fixes, if any, were required.** <br>\n",
    "\n",
    "**g. Specify any additional features that youâ€™d like to point out.** <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable                          Type        Data/Info\n",
      "-------------------------------------------------------\n",
      "ET                                module      <module 'xml.etree.Elemen<...>\\\\etree\\\\ElementTree.py'>\n",
      "IPython                           module      <module 'IPython' from 'C<...>s\\\\IPython\\\\__init__.py'>\n",
      "Sen_Probability                   function    <function Sen_Probability at 0x000002070F75C0D0>\n",
      "a                                 int         10\n",
      "ast                               module      <module 'ast' from 'C:\\\\U<...>\\anaconda3\\\\lib\\\\ast.py'>\n",
      "b                                 str         hello\n",
      "build_and_dump                    function    <function build_and_dump at 0x0000020732944A60>\n",
      "c                                 list        n=3\n",
      "calculate_laplace                 function    <function calculate_lapla<...>ce at 0x0000020732944040>\n",
      "calculate_n_gram                  function    <function calculate_n_gram at 0x0000020732941EE0>\n",
      "calculate_perplexity              function    <function calculate_perpl<...>ty at 0x000002073290E550>\n",
      "calculate_probability             function    <function calculate_proba<...>ty at 0x00000207328DACA0>\n",
      "calculate_unk                     function    <function calculate_unk at 0x00000207329580D0>\n",
      "calculate_vanilla                 function    <function calculate_vanil<...>la at 0x0000020732941940>\n",
      "choose                            function    <function choose at 0x000002070F7BE550>\n",
      "dump_data                         function    <function dump_data at 0x0000020739DE6160>\n",
      "extract                           function    <function extract at 0x0000020732944E50>\n",
      "filter_bi_tr                      function    <function filter_bi_tr at 0x00000207619503A0>\n",
      "generate                          function    <function generate at 0x000002070F7BE1F0>\n",
      "json                              module      <module 'json' from 'C:\\\\<...>\\lib\\\\json\\\\__init__.py'>\n",
      "linear_interpolation_perplexity   function    <function linear_interpol<...>ty at 0x0000020732941F70>\n",
      "linear_interpolation_sentence     function    <function linear_interpol<...>ce at 0x0000020732941A60>\n",
      "my_tuple                          tuple       n=2\n",
      "name                              str         _i395\n",
      "open_data                         function    <function open_data at 0x0000020739DE6F70>\n",
      "os                                module      <module 'os' from 'C:\\\\Us<...>\\\\anaconda3\\\\lib\\\\os.py'>\n",
      "random                            module      <module 'random' from 'C:<...>aconda3\\\\lib\\\\random.py'>\n",
      "raw_data                          function    <function raw_data at 0x000002073290E1F0>\n",
      "smooth                            function    <function smooth at 0x000002073290EC10>\n",
      "sort_dict_showing_items           function    <function sort_dict_showi<...>ms at 0x000002070F75C8B0>\n",
      "sys                               module      <module 'sys' (built-in)>\n",
      "text                              str         ('<s>', 'Oh')\n",
      "time                              module      <module 'time' (built-in)>\n",
      "val                               str         for name, val in IPython.<...>t(f\"{name}: {type(val)}\")\n",
      "x                                 tuple       n=1\n"
     ]
    }
   ],
   "source": [
    "import IPython\n",
    "IPython.get_ipython().magic('whos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
